{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "import torch\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet, NaNLabelEncoder\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, QuantileLoss, MAPE\n",
    "import locale\n",
    "import pickle\n",
    "locale.setlocale(locale.LC_ALL, 'it_IT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_settimanale = pd.read_csv(\"processed_data/first_dataset.csv\")\n",
    "df_settimanale[\"time_idx\"] = df_settimanale[\"time_idx\"].astype(int)\n",
    "df_settimanale[\"azoto_oraria_max\"] = df_settimanale[\"azoto_oraria_max\"].astype(float)\n",
    "df_settimanale[\"zolfo_giornaliera\"] = df_settimanale[\"zolfo_giornaliera\"].astype(float)\n",
    "df_settimanale[\"ozono_8h_max\"] = df_settimanale[\"ozono_8h_max\"].astype(float)\n",
    "df_settimanale[\"pm2dot5_giornaliera\"] = df_settimanale[\"pm2dot5_giornaliera\"].astype(float)\n",
    "df_settimanale[\"pm10_giornaliera\"] = df_settimanale[\"pm10_giornaliera\"].astype(float)\n",
    "df_settimanale[\"provincia\"] = df_settimanale[\"provincia\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_settimanale=df_settimanale.drop([\"Unnamed: 0\"], axis=1)\n",
    "df_settimanale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provo ad aumentare l'encoder length, che dovrebbe dirmi, semplificando, \"quanto guardare indietro\" per la previsione.\n",
    "Si potrebbero inoltre aggiungere features numeriche statiche come media e deviazione standard per almeno il pm10, per ogni provincia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 4\n",
    "max_encoder_length = 32\n",
    "training_cutoff = df_settimanale[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df_settimanale[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"pm10_giornaliera\",\n",
    "    group_ids=[\"provincia\"],\n",
    "    min_encoder_length=max_encoder_length, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=max_prediction_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"provincia\"],\n",
    "    static_reals=[], # da agginugere media e dev_std del pm10\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_known_reals=[\"time_idx\",\"relative_time_w\",\"relative_time_y\",\"relative_time_m\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"azoto_oraria_max\",\"zolfo_giornaliera\",\n",
    "        \"ozono_8h_max\",\"pm2dot5_giornaliera\",\"pm10_giornaliera\", \"impianti_rifiuti\", \"siti_inquinati\"\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"provincia\"], transformation=\"relu\", scale_by_group=True\n",
    "    ),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(training, df_settimanale, predict=True, stop_randomization=True)\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*4, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolo delle performance della baseline, utili per verificare i risultati dell'algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "baseline_model = Baseline()\n",
    "baseline_predictions = baseline_model.predict(val_dataloader)\n",
    "maPe = mean_absolute_percentage_error(actuals, baseline_predictions)\n",
    "maPe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli iperparametri sono ottimizzati tramite optuna. Inoltre si è visto graficamente che dopo l'epoca 67 il modello va in overfitting.\n",
    "Il miglior modello ottenuto è version_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=15, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=67,\n",
    "    gpus=0,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=0.39993629374404666,\n",
    "    callbacks=[lr_logger],\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.06130995353990287,\n",
    "    hidden_size=117,  # most important hyperparameter apart from learning rate\n",
    "    attention_head_size=4,# number of attention heads. Set to up to 4 for large datasets\n",
    "    dropout=0.2,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=43,  # set to <= hidden_size\n",
    "    output_size=7,  \n",
    "    loss=QuantileLoss(),\n",
    "    reduce_on_plateau_patience=50,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=200,\n",
    "    max_epochs=50,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "\n",
    "maPe = mean_absolute_percentage_error(actuals, predictions)\n",
    "maPe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "\n",
    "\n",
    "for idx in range(4):\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, x = best_tft.predict(val_dataloader, return_x=True)\n",
    "predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(x, predictions)\n",
    "best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation = best_tft.interpret_output(raw_predictions, reduction=\"sum\")\n",
    "best_tft.plot_interpretation(interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provincia = \"Trieste\"\n",
    "\n",
    "raw_prediction, xraw = best_tft.predict(\n",
    "    training.filter(lambda x: (x.provincia == provincia)),\n",
    "    mode=\"prediction\",\n",
    "    return_x=True,\n",
    ")\n",
    "baseline_predictions = Baseline().predict(training.filter(lambda x: (x.provincia == provincia)),\n",
    "    mode=\"prediction\",\n",
    "    return_x=False)\n",
    "\n",
    "predicted_df = pd.DataFrame([],columns=[\"time_idx\",\"values\",\"baseline_values\"])\n",
    "i=0\n",
    "while i < len(raw_prediction):\n",
    "    for x in range(0,len(raw_prediction[i])):\n",
    "        predicted_df = predicted_df.append({\"time_idx\":xraw[\"decoder_time_idx\"][i][x],\"values\":raw_prediction[i][x].numpy(),\"baseline_values\":baseline_predictions[i][x].numpy()},ignore_index=True)\n",
    "    i += len(raw_prediction[i])\n",
    "predicted_df[\"values\"] = predicted_df[\"values\"].astype(float)\n",
    "predicted_df[\"baseline_values\"] = predicted_df[\"baseline_values\"].astype(float)\n",
    "predicted_df = predicted_df.groupby(by=\"time_idx\").mean().reset_index()\n",
    "target = df_settimanale[df_settimanale[\"provincia\"]==provincia]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=target[\"time_idx\"], y=target[\"pm10_giornaliera\"],mode='lines+markers',name=\"Realtà\"))\n",
    "fig.add_trace(go.Scatter(x=predicted_df[\"time_idx\"], y=predicted_df[\"baseline_values\"],mode='lines+markers',name=\"Baseline\"))\n",
    "fig.add_trace(go.Scatter(x=predicted_df[\"time_idx\"], y=predicted_df[\"values\"],mode='lines+markers',name=\"Predizione\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provincia = \"Trieste\"\n",
    "raw_prediction, xraw = best_tft.predict(\n",
    "    validation.filter(lambda x: (x.provincia == provincia)),\n",
    "    mode=\"prediction\",\n",
    "    return_x=True,\n",
    ")\n",
    "predicted_df = pd.DataFrame([],columns=[\"time_idx\",\"values\"])\n",
    "i=0\n",
    "while i < len(raw_prediction):\n",
    "    for x in range(0,len(raw_prediction[i])):\n",
    "        predicted_df = predicted_df.append({\"time_idx\":xraw[\"decoder_time_idx\"][i][x],\"values\":raw_prediction[i][x].numpy()},ignore_index=True)\n",
    "    i += len(raw_prediction[i])\n",
    "predicted_df[\"values\"] = predicted_df[\"values\"].astype(float)\n",
    "target = df_settimanale[df_settimanale[\"provincia\"]==provincia]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=target[\"time_idx\"], y=target[\"pm10_giornaliera\"],mode='lines+markers',name=\"Realtà\"))\n",
    "fig.add_trace(go.Scatter(x=predicted_df[\"time_idx\"], y=predicted_df[\"values\"],mode='lines+markers',name=\"Predizione\"))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e8ba394dfecf69ab3b214a23e7e279956d3407d86cf3cc4fe5cfb6d1f0e6e68"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
